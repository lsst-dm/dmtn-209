\section{Introduction}

The Legacy Survey of Space and Time is an optical/near-IR survey of half the sky in ugrizy bands to r 27.5 (36 nJy) based on 825 visits over a 10-year period: deep wide fast.
Carried out by Rubin observatory on Cerro Pcon Chile the survey will produce around 100 PB of data consisting of about a billion 16 Mpix images, enabling measurements for 40 billion objects! \cite{arXiv:0805.2366}

We have recently begun a pre operations program which includes exposing users to the Science Platform.


\section{Interim Data Facility on Google }
We decided on an Interim Data Facility (IDF) to mitigate the risks of — and the delay imposed by — the USDF(US Data Facility)  selection process.
We felt this an excellent transition activity to prepare Ops team and Community for Rubin data.
In a competitive tender we selected google for this three year activity.
We have installed construction delivered services (Rubin Science Platform and middleware/plumbing) on Google Cloud Platform.
The architecture is depicted in Figure \ref{fig:IDFarch}.
We primarily leverage Google Kubernetes Engine (GKE) and Google Cloud Storage (GCS) to scale up and down as needed while batch processing images from the observatory and serving the data release products.
The full stack was automated using Terraform with help from Burwood, which is not highlighted in the diagram but is outlined in section \ref{sec:infra}.
In addition running services at scale improves readiness for Data Production activity during full survey operations.




\articlefigure[width=0.9\textwidth]{IDFarch.eps}{fig:IDFarch}{Architecture of the Interim Data Facility on Google, gray Items (alerts) are not yet implemented but shall be in 2022.}



\subsection{ Docker, Kubernetes, Terraform} \label{sec:infra}
The setup of the infrastructure such as clusters on GCP is done using Terraform\footnote{\url{https://www.terraform.io/}}.
Typically we have development, integration and production clusters for each major deployment e.g. Science Platform and Qserv.
The orchestration of the Rubin application containers is done using Kubernetes\footnote{\url{https://kubernetes.io/}} with deployment handled by ArgoCD\footnote{\url{https://argoproj.github.io/cd/}}
Our build system, based on Jenkins and GitHub Actions,  produces Docker containers for deployment.

\section {Data Previews}
The IDF will be used for a series of Data Previews (DP). DP0 is using simulated data, DP1 commissioning camera data
and DP2 LSST Cam commissioning data. DP0 currently has two phases as outlined in Sections \ref{sec:dp0} and \ref {sec:dp1}.

\subsection{Data Preview 0.1}\label{sec:dp01}
In June 2021  \href{  data.lsst.cloud}{data.lsst.cloud} was opened up to the first 200 delegates.
Documentation is hosted on \href{ dp0-1.lsst.io}{ dp0-1.lsst.io}.
The data served was a the DESC DC2 simulated data \citep{arXiv:2010.05926}.



\subsection{Data Preview 0.2}\label{sec:dp02}
For DP0.2 we shall reprocess the DESC data (DP0.1) with Pipelines V23, to generate a fully self-consistent data release.
This will entail producing all the data products and catalogs as well as loading the  catalogs in Qserv.
Thus demonstrating a portable set of cloud enabled tools based on Butler Gen3 and PanDA.


\section{Community engagement }
The Rubin Community Engagement Team (CET) provided documentation and support to the scientists and students participating in DP0 as delegates.
The term 'DP0 delegates' was adopted to reflect DP0 participants’ important roles of representing the broad science community as learners, testers, and providers of feedback, and of sharing the benefits of their DP0 participation with their communities as teachers and colleagues.

Documentation at dp0-1.lsst.io included a delegate's homepage listing all the resources, events, guidelines, and expectations; descriptions of the data products and table schema; instructions for accessing the RSP's three aspects and usage risks; and tutorials to demonstrate simple science investigations with the DP0 data set at the RSP. 
Delegate resources included biweekly virtual “Delegate Assemblies” with hands-on demonstrations and breakout rooms to facilitate co-working; a dedicated category in the Rubin Community Forum (Community.lsst.org) where delegates can ask questions and discuss their DP0 work; and a GitHub repository where delegates can contribute to shared code and notebooks.
GitHub Issues were also used as a means for delegates to request technical assistance.

Rubin staff from the CET and Data Production worked together to respond to delegate's requests for support and bug reports.
All of the delegate resources are publicly accessible to non-delegates, including the assemblies (and their recordings) and the Community Forum discussions.

\section{Conclusion and lessons learned}
Users need a lot of guidance if we are to be efficient.
For example Qserv (ADQL) can build great histograms (minutes for full dataset) but users
tended to do this in python by running a query to extract all the data and ten binning in python.
AutoScale really works - but its not really fast considering our containers, it takes minutes to spin up new node - eons as far as users concerned!
DASK looks very interesting but we are not sure how to mange that yet learning to be done.
No matter how much you prepare you always learn something new!

Management buy in is a big part of any cloud adventure, there is a  lot of reluctance to change from in house models
which requires much convincing. One thing we did were proof on concept studies with both Amazon \citep{DMTN-137} and Google \citep{DMTN-125}. Each ran for six months and demonstrated we could deploy and operate on either platform. This allayed concerns about vendor lockin and improved cost models.

Since we built cloud ready services setup on GCP was fairly painless and smooth
Support from Google and Burwood was excellent and rapid, weekly tag-ups continue to make sure everything is in place or planned.
Since Rubin is between data facilities for some years this is a quite cost effective approach.






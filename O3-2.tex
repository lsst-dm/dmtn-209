\documentclass[11pt,twoside]{article}

% Do NOT use ANY packages other than asp2014.
\usepackage{asp2014}

\aspSuppressVolSlug
\resetcounters

% References must all use BibTeX entries in a .bibfile.
% References must be cited in the text using \citet{} or \citep{}.
% Do not use \cite{}.
% See ManuscriptInstructions.pdf for more details
\bibliographystyle{asp2014}
\def\procspie{Proc.\ SPIE} % Proceedings of the SPIE


% The ``markboth'' line sets up the running heads for the paper.
% 1 author: "Surname"
% 2 authors: "Surname1 and Surname2"
% 3 authors: "Surname1, Surname2, and Surname3"
% >3 authors: "Surname1 et al."
% Replace ``Short Title'' with the actual paper title, shortened if necessary.
% Use mixed case type for the shortened title
% Ensure shortened title does not cause an overfull hbox LaTeX error
% See ASPmanual2010.pdf 2.1.4  and ManuscriptInstructions.pdf for more details
\markboth{O'Mullane, Economou}{Rubin Science Platform on Google Title}

\begin{document}

\title{Rubin Science Platform on Google: the story so far.}

% Note the position of the comma between the author name and the
% affiliation number.
% Authors surnames should come after first names or initials, eg John Smith, or J. Smith.
% Author names should be separated by commas.
% The final author should be preceded by "and".
% Affiliations should not be repeated across multiple \affil commands. If several
% authors share an affiliation this should be in a single \affil which can then
% be referenced for several author names. If only one affiliation, no footnotes are needed.
% See ManuscriptInstructions.pdf and ASP's manual2010.pdf 3.1.4 for more details
\input{authors}

% There should be one \aindex line (commented out) for each author. These are used to
% build up the author index for the Proceedings. The surname must come first, followed by
% initials. Note the use of ~ before each initial to control spacing.
% The \author entries (see above) have surname last. These \aindex entries have
% surname first.
% The Aindex.py command willl create them for you after you have constructed the \author
% The first entry should be the first author, for bold-facing the author index page-reference

%\aindex{FistAuthor1,~S.~A.}
%\aindex{Author2,~S.~B.}
%\aindex{Author3,~S.}


\input{abstract}

%Slides on https://docs.google.com/presentation/d/15XiQawtQ4aZCwU1qQ6seXXg6jTfLU7j7K9pjh7ReucE/edit#slide=id.ged99d87c95_0_165
\section{Introduction}

The Legacy Survey of Space and Time \citep{2019ApJ...873..111I} is "deep wide fast" optical/near-IR survey of half the sky in ugrizy bands to r 27.5 (36 nJy) based on 825 visits over a 10-year period.
Carried out by Rubin observatory on Cerro Pach\'{o}n Chile, the survey will produce around 100\,PB of data consisting of about a billion 16\,Mpix images, enabling measurements for 40 billion objects!

As a big data project, Rubin is providing a "bring-your-compute-to-the-data" model relying on a project-provided Rubin Science Platform (hereafter RSP) to allow science users to access, visualize and analyze survey data products.
This paper describes our experience with an early release of the RSP to users through our pre-operations program.

\section{The Rubin Interim Data Facility on Google Cloud}

We decided to deploy services involved in our pre-operations program on a commodity cloud services provider as an Interim Data Facility (IDF) to mitigate uncertainty in the final location of our operations-era US Data Facility (USDF).
Following a competitive tender, we selected Google Cloud for this three-year activity to prepare our team for operational readiness and our community for interacting with Rubin data and services.
The IDF is a full stand-alone facility (without dependance on on-premises computing) and is a real high-availability production environment for all services currently oriented to external users. These include the Rubin Science Platform as well as underlying services such as Qserv (our high performance database system) and Butler (data abstraction middleware).

The architecture is depicted in Figure \ref{fig:IDFarch} and leverages the Google Kubernetes Engine (GKE) and Google Cloud Storage (GCS) to scale up and down as needed depending on demand.
Kubernetes is our reference deployment platform both on-premises and in the Cloud as our services make heavy use of container orchestration patterns and consume containers produced by our build systems based on Jenkins and GitHub Actions \citep[see e.g.,][]{2018SPIE10707E..09J}.

Management of Google Cloud resources is fully captured in code (Terraform) developed by the Burwood Group and no production services rely on ad-hoc interactive use of the Google console. We use continuous deployment techniques to manage our services including tools such as ArgoCD\footnote{\url{https://argoproj.github.io/cd/}} even though we don't practice pure continuous deployment, having instead settled on a gated release model \citep{SQR-056}.

\articlefigure[width=0.9\textwidth]{IDFarch.eps}{fig:IDFarch}{Architecture of the Interim Data Facility on Google, gray Items (alerts) are not yet implemented but shall be in 2022.}

\section {The Pre-operations Data Preview programme}
The IDF is hosting what is the first of a series of Data Previews (DP). DP0 is using simulated data (as the telescope has not yet achieved First Light), DP1 will offer commissioning camera data,
and DP2 LSSTCam commissioning data. DP0 currently has two phases\citep{RTN-001} with one focussing on data access and the other on data processing readiness.
In June 2021 \href{https://data.lsst.cloud}{data.lsst.cloud} was opened up to the first 200 delegates selected as part of Data Preview 0.1 with data from the DC2 simulated dataset provided by the DESC collaboration \citep{arXiv:2010.05926}.
For Data Preview 0.2 we shall reprocess the DESC data published "as is" in DP0.1 with the LSST Science Pipelines v23, to generate a fully self-consistent data release that matches the LSST data model.
This will entail producing many types of the final data products and catalogs as well as loading the  generated catalogs in Qserv and will demonsrate readiness (and on-prem/cloud portability) for a number of systems including our PanDA-based processing system and the Generation 3 version of the Butler \citep{2019ASPC..523..653J}.

\section{Community Engagement }

Besides the technical and operational goals, a key aim of deployment of the Interim Data Facility was the engagement of the future community of scientists and students by providing early access to ``DP0 delegates'',a group of users representing the broad science community as learners, testers, and providers of feedback, and tasked with sharing the benefits of their DP0 participation with their communities.

Besides extensive document at \href{https://dp0-1.lsst.io}{dp0-1.lsst.io} supporting delegates in a number of ways, delegate resources included biweekly virtual ``Delegate Assemblies'' with hands-on demonstrations and breakout rooms to facilitate co-working; a dedicated category in the Rubin Community Forum (\url{https://community.lsst.org}) (based on the Discourse web forum platform) where delegates can ask questions and discuss their DP0 work; and a GitHub repository where delegates can contribute to shared code and notebooks.
GitHub Issues were also used as the primary means for delegates to request technical assistance. All of the delegate pedagogical resources are publicly accessible.

Some aspects of the Community Engagement model as applied to DP0 are designed to scale to thousands of users for the future Data Previews, such as the extensive documentation, the tutorials, and the use of the Community Forum.
Other aspects, such as the frequent live virtual sessions, may not scale well to thousands of users and were specifically designed to ramp up skill levels from novice to intermediate in order to seed expertise with Rubin software in the community.
For future Data Previews, the Community Engagement strategy will focus on building infrastructure to foster a vibrant science community, that enables self-help, peer-to-peer help and crowdsourced support.

\section{Cloud as a primary platform for astronomical data services}

The successful launch of our Data Preview program with our cloud-based Science Platform reinforced our view that commodity infrastructure offers unparalleled technical and programmatic advantages in delivering large-scale data services to astronomers. Some key points:

\begin{itemize}

    \item Architecting systems using cloud-native techniques (such as targetting Kubernetes as the deployment platform) allows painless transition between commodity and on-premises infrastructure.
    \item There are tangible benefits to working with a highly popular toolchain both for a project and its developers, including the ease of contracting short-term bootstrapping help as Rubin did with the Burwood Group.
    \item Developers love the self-serve aspect of cloud infrastructures (and their velocity reflects this).
    \item Use of commercial cloud environments offers an ideal way to mitigate schedule and technical risk in the deliver of on-premises computing.
    \item There are significant security advantages in placing public users in a separate security domain from on-premises resources.
    \item We are seriously evaluating whether an on-prem/cloud hybrid model is actually the best way forward permanently for Rubin Operations.
\end{itemize}

While use of these services are not cheap (despite academic and government discounts that can be had for the asking), they offer great value for money (and are frequently disadvantaged in comparisons with on-premises cost by the lack of total cost accounting of in-house facilities).
Management buy-in is a big part of any cloud adventure; in our case we were helped by our proof of concept studies with both Amazon \citep{2020arXiv201106044B,DMTN-137} and Google \citep{DMTN-125}. Each ran for six months and demonstrated we could deploy and operate on either platform. This allayed concerns about vendor lock-in and improved our cost models.

We believe use of commodity cloud infrastructure will allow scientific facilities to focus on fertile domain-relevant middleware development instead of on-premises compute facilities and we hope that the funding landscape can evolve to facilitate this paradigm shift.
\bibliography{O3-2}  % For BibTex

\end{document}
